
## Importing packages:
```{r}
library(keras)
library(tensorflow)
```

## Data URLS (in system):
```{r}
base_dir <- "D:/my_work/College/sem6/data analytics/project/project main/with_and_without_masks_grayscale"
train_dir <- file.path(base_dir, "train")
validation_dir <- file.path(base_dir, "validation")
test_dir <- file.path(base_dir, "test")
```


## Image Generator:
```{r}
## image generator

train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

test_datagen <- image_data_generator(rescale = 1/255)  

train_generator <- flow_images_from_directory(
  train_dir,                  # Target directory  
  train_datagen,              # Data generator
  target_size = c(150, 150),  # Resizes all images to 150 Ã— 150
  batch_size = 20,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

```


## VGG16 Model
```{r}

# Initiate VGG-16 model
# install_tensorflow()
# install_keras()
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
summary(conv_base)

#Feature extraction with data augmentation
model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
summary(model)

length(model$trainable_weights)

freeze_weights(conv_base)
length(model$trainable_weights)


model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)


#fine tuning
summary(conv_base)
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)


## Adam:

## freeze
freeze_weights(conv_base)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(0.001),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
## fine tuning
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(0.001),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)

```

## Resnet50 Model
```{r}
## resnet50 model
model = application_resnet50(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3))

## model sequential
model <- keras_model_sequential() %>% 
  model %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
summary(model)

length(model$trainable_weights)

freeze_weights(conv_base)
length(model$trainable_weights)

### using rmsprop
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
#fine tuning
summary(conv_base)
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)

### using adam optimiser
freeze_weights(conv_base)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
#fine tuning
summary(conv_base)
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
```

## Inceptionv3 Model:
```{r}

conv_base = application_inception_v3( 
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3))

summary(conv_base)

#Feature extraction with data augmentation
model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
summary(model)

length(model$trainable_weights)

freeze_weights(conv_base)
length(model$trainable_weights)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)


#fine tuning
summary(conv_base)
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)

## Adam:

## freeze
freeze_weights(conv_base)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
## fine tuning
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
```

## Xception Model:
```{r}

conv_base = application_xception(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)

summary(conv_base)

#Feature extraction with data augmentation
model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
summary(model)

length(model$trainable_weights)

freeze_weights(conv_base)
length(model$trainable_weights)

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)


#fine tuning
summary(conv_base)
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)

## Adam:

## freeze
freeze_weights(conv_base)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
## fine tuning
unfreeze_weights(conv_base, from = "block3_conv1")
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(1e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)

```

```{r}
### based on time duration, saving Inceptionv3 model with adam optimizer.

conv_base = application_inception_v3( 
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3))

summary(conv_base)

#Feature extraction with data augmentation
model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
## Adam:

## freeze
freeze_weights(conv_base)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = validation_generator,
  validation_steps = 50
)
saveRDS(model, "./final_model.rds")
```

